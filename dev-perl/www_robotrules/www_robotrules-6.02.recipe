SUMMARY="Database of robots.txt-derived permissions"
DESCRIPTION="This module parses /robots.txt files as specified in \"A Standard for Robot \
Exclusion\", at <http://www.robotstxt.org/wc/norobots.html> Webmasters can use the /robots.txt \
file to forbid conforming robots from accessing parts of their web site.
The parsed files are kept in a WWW::RobotRules object, and this object provides methods to check \
if access to a given URL is prohibited. The same WWW::RobotRules object can be used for one or \
more parsed /robots.txt files on any number of hosts."
HOMEPAGE="https://metacpan.org/pod/WWW::RobotRules"
COPYRIGHT="1995-2009, Gisle Aas
	1995, Martijn Koster"
LICENSE="Artistic"
REVISION="1"
SOURCE_URI="https://cpan.metacpan.org/authors/id/G/GA/GAAS/WWW-RobotRules-$portVersion.tar.gz"
CHECKSUM_SHA256="46b502e7a288d559429891eeb5d979461dd3ecc6a5c491ead85d165b6e03a51e"
SOURCE_DIR="WWW-RobotRules-$portVersion"

ARCHITECTURES="any"

PROVIDES="
	www_robotrules = $portVersion
	"
REQUIRES="
	haiku
	uri
	vendor_perl
	"

BUILD_REQUIRES="
	haiku_devel
	"
BUILD_PREREQUIRES="
	cmd:make
	cmd:perl
	"

TEST_REQUIRES="
	uri
	"

BUILD()
{
	perl Makefile.PL PREFIX=$prefix
	make
}

INSTALL()
{
	make pure_install

	# remove architecture-specific files
	cd $prefix
	rm -r $(perl -V:vendorarch | cut -d\' -f2 | cut -d/ -f5-)
		# cut extracts the quoted string and strips the prefix (which is perl's and not ours)
}

TEST()
{
	make test
}
