SUMMARY="Inference of Meta's LLaMA model (and others) in pure C/C++"
DESCRIPTION="The main goal of llama.cpp is to enable LLM inference with minimal setup and state-of-the-art \
performance on a wide variety of hardware - locally and in the cloud.

* Plain C/C++ implementation without any dependencies
* AVX and AVX2 support for x86 architectures
* 1.5-bit, 2-bit, 3-bit, 4-bit, 5-bit, 6-bit, and 8-bit integer quantization for faster inference and reduced memory use

Since its inception, the project has improved significantly thanks to many contributions. It is the main playground \
for developing new features for the ggml library."
HOMEPAGE="https://github.com/ggerganov/llama.cpp/"
COPYRIGHT="Georgi Gerganov"
LICENSE="MIT"
REVISION="1"
SOURCE_URI="https://github.com/ggerganov/llama.cpp/archive/refs/tags/$portVersion.tar.gz"
CHECKSUM_SHA256="ab1cf3d8f913534bf8d35bf1bd93db45d1692654ee8c14799d415661d15f9848"
SOURCE_DIR="llama.cpp-$portVersion"
SOURCE_URI_2="https://github.com/microsoft/mimalloc/archive/refs/tags/v2.1.7.tar.gz"
CHECKSUM_SHA256_2="0eed39319f139afde8515010ff59baf24de9e47ea316a315398e8027d198202d"
SOURCE_DIR_2="mimalloc-2.1.7"

PATCHES="llama_cpp-$portVersion.patchset"

ARCHITECTURES="all !x86_gcc2"

PROVIDES="
	llama_cpp = $portVersion
	lib:libggml = $portVersion
	lib:libggml_base = $portVersion
	lib:libggml_cpu = $portVersion
	lib:libggml_rpc = $portVersion
	lib:libllama = $portVersion
	lib:libllava_shared = $portVersion
	cmd:convert_hf_to_gguf.py
	cmd:llama_baby_llama
	cmd:llama_batched
	cmd:llama_batched_bench
	cmd:llama_bench
	cmd:llama_cli
	cmd:llama_convert_llama2c_to_ggml
	cmd:llama_cvector_generator
	cmd:llama_embedding
	cmd:llama_eval_callback
	cmd:llama_export_lora
	cmd:llama_gbnf_validator
	cmd:llama_gguf
	cmd:llama_gguf_hash
	cmd:llama_gguf_split
	cmd:llama_gen_docs
	cmd:llama_gritlm
	cmd:llama_imatrix
	cmd:llama_infill
	cmd:llama_llava_cli
	cmd:llama_llava_clip_quantize_cli
	cmd:llama_lookahead
	cmd:llama_lookup
	cmd:llama_lookup_create
	cmd:llama_lookup_merge
	cmd:llama_lookup_stats
	cmd:llama_minicpmv_cli
	cmd:llama_parallel
	cmd:llama_passkey
	cmd:llama_perplexity
	cmd:llama_quantize
	cmd:llama_quantize_stats
	cmd:llama_qwen2vl_cli
	cmd:llama_retrieval
	cmd:llama_run
	cmd:llama_save_load_state
	cmd:llama_server
	cmd:llama_simple
	cmd:llama_simple_chat
	cmd:llama_speculative
	cmd:llama_speculative_simple
	cmd:llama_tokenize
	cmd:llama_tts
	cmd:rpc_server
	"
REQUIRES="
	haiku
	lib:libgomp
	"

PROVIDES_devel="
	llama_cpp_devel = $portVersion
	devel:libggml = $portVersion
	devel:libggml_base = $portVersion
	devel:libggml_cpu = $portVersion
	devel:libggml_rpc = $portVersion
	devel:libllama = $portVersion
	devel:libllava_shared = $portVersion
	"
REQUIRES_devel="
	llama_cpp == $portVersion base
	"

BUILD_REQUIRES="
	haiku_devel
	devel:libgomp
	"
BUILD_PREREQUIRES="
	cmd:cmake
	cmd:make
	cmd:gcc
	cmd:git
	"

BUILD()
{
	cmake -Bbuild_mimalloc -S$sourceDir2 -DCMAKE_BUILD_TYPE=Release \
		-DMI_BUILD_SHARED=OFF \
		-DMI_BUILD_STATIC=OFF \
		-DMI_BUILD_TESTS=OFF \
		-DMI_BUILD_OBJECT=ON

	make -C build_mimalloc $jobArgs

	cmake -Bbuild -S. -DCMAKE_BUILD_TYPE=Release \
		$cmakeDirArgs \
		-DBUILD_SHARED_LIBS_DEFAULT=ON \
		-DLLAMA_BUILD_SERVER=ON \
		-DLLAMA_BUILD_TESTS=OFF \
		-DLLAMA_BUILD_EXAMPLES=ON \
		-DGGML_NATIVE=OFF \
		-DGGML_RPC=ON \
		-DCMAKE_EXE_LINKER_FLAGS="$sourceDir/build_mimalloc/mimalloc.o"

	make -C build $jobArgs
}

INSTALL()
{
	make -C build install
	cp build/bin/rpc-server $binDir

	prepareInstalledDevelLibs \
		libggml \
		libggml-base \
		libggml-cpu \
		libggml-rpc \
		libllama \
		libllava_shared

	fixPkgconfig

	packageEntries devel \
		$developDir \
		$libDir/cmake
}

TEST()
{
	make -C build test
}
